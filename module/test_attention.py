"""
Tests generated by Claude Code.
"""

import jax
import jax.numpy as jnp
import pytest
import flax.linen as nn
from .attention import (
    scaled_dot_product_attention,
    MASK_VALUE,
    MultiHeadAttention,
)


class TestScaledDotProductAttention:
    def test_basic_functionality(self):
        """Test basic attention computation with simple inputs."""
        batch_size, num_heads, seq_len, head_dim = 2, 4, 8, 16

        # Create test inputs
        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )

        # No masking (all True)
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Check output shape
        expected_shape = (batch_size, num_heads, seq_len, head_dim)
        assert result.shape == expected_shape

        # Check that result is finite
        assert jnp.all(jnp.isfinite(result))

    def test_output_shape_consistency(self):
        """Test that output shape is consistent across different input sizes."""
        test_cases = [
            (1, 2, 4, 8),  # small
            (2, 8, 12, 32),  # medium
            (4, 16, 8, 64),  # large
        ]

        for batch_size, num_heads, seq_len, head_dim in test_cases:
            q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            v = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

            result = scaled_dot_product_attention(q, k, v, mask)
            expected_shape = (batch_size, num_heads, seq_len, head_dim)
            assert result.shape == expected_shape

    def test_attention_scaling(self):
        """Test that attention scores are properly scaled by sqrt(head_dim)."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create inputs with known values
        q = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]]])
        k = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]]])
        v = jnp.array([[[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # With orthogonal q and k, attention should be uniform after softmax
        # Check that result is reasonable
        assert jnp.all(jnp.isfinite(result))
        assert result.shape == (1, 1, 2, 4)

    def test_causal_masking(self):
        """Test causal (lower triangular) masking."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 3, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )

        # Create causal mask (lower triangular)
        causal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))
        mask = jnp.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (batch_size, num_heads, seq_len, head_dim)
        assert jnp.all(jnp.isfinite(result))

    def test_full_masking(self):
        """Test behavior with fully masked attention."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = jnp.ones((batch_size, num_heads, seq_len, head_dim))

        # All positions masked out
        mask = jnp.zeros((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Result should still have correct shape
        assert result.shape == (batch_size, num_heads, seq_len, head_dim)

        # With all positions masked, softmax should produce uniform weights
        # over masked positions, but the exact behavior depends on implementation
        assert jnp.all(jnp.isfinite(result))

    def test_identity_attention(self):
        """Test attention when q, k, v are designed for identity mapping."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create inputs where each position attends only to itself
        q = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]]])
        k = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]]])
        v = jnp.array([[[[10.0, 20.0, 30.0, 40.0], [50.0, 60.0, 70.0, 80.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (1, 1, 2, 4)
        assert jnp.all(jnp.isfinite(result))

    def test_batch_independence(self):
        """Test that different batches are processed independently."""
        batch_size, num_heads, seq_len, head_dim = 3, 2, 2, 4

        # Create different inputs for each batch
        q = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )
        k = q + 1.0
        v = q + 2.0

        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Process each batch individually and compare
        for i in range(batch_size):
            single_result = scaled_dot_product_attention(
                q[i : i + 1], k[i : i + 1], v[i : i + 1], mask[i : i + 1]
            )
            assert jnp.allclose(result[i : i + 1], single_result, rtol=1e-5)

    def test_attention_weights_sum_to_one(self):
        """Test that attention weights sum to 1 for unmasked positions."""
        batch_size, num_heads, seq_len, head_dim = 1, 1, 3, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = jnp.ones((batch_size, num_heads, seq_len, head_dim))

        # Test with partial masking
        mask = jnp.array(
            [[[True, True, False], [True, True, True], [True, False, True]]]
        )

        # We can't directly test attention weights since they're internal,
        # but we can test that the function runs correctly
        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (batch_size, num_heads, seq_len, head_dim)
        assert jnp.all(jnp.isfinite(result))

    def test_different_qkv_values(self):
        """Test attention with different q, k, v values."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create distinct q, k, v
        q = jnp.array([[[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]]])
        k = jnp.array([[[[2.0, 1.0, 4.0, 3.0], [6.0, 5.0, 8.0, 7.0]]]])
        v = jnp.array([[[[10.0, 20.0, 30.0, 40.0], [50.0, 60.0, 70.0, 80.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (1, 1, 2, 4)
        assert jnp.all(jnp.isfinite(result))

        # Result should be different from input values
        assert not jnp.allclose(result, q)
        assert not jnp.allclose(result, k)
        assert not jnp.allclose(result, v)

    def test_mask_value_constant(self):
        """Test that MASK_VALUE is properly defined."""
        assert MASK_VALUE == -1e10
        assert isinstance(MASK_VALUE, (int, float))
        assert MASK_VALUE < 0


class TestMultiHeadAttention:
    def test_initialization(self):
        """Test MultiHeadAttention initialization."""
        num_heads, head_dim = 8, 64
        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)

        # Check attributes
        assert model.num_heads == num_heads
        assert model.head_dim == head_dim

    def test_basic_forward_pass(self):
        """Test basic forward pass through MultiHeadAttention."""
        batch_size, seq_len, num_heads, head_dim = 2, 4, 8, 32
        hidden_size = num_heads * head_dim

        # Initialize model
        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)

        # Create inputs
        x = jnp.ones((batch_size, seq_len, hidden_size))
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        # Initialize parameters
        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask)

        # Forward pass
        output = model.apply(params, x, mask)

        # Check output shape
        assert output.shape == (batch_size, seq_len, hidden_size)
        assert jnp.all(jnp.isfinite(output))

    def test_different_sizes(self):
        """Test MultiHeadAttention with different input sizes."""
        test_cases = [
            (1, 2, 4, 16),  # small
            (2, 8, 12, 32),  # medium
            (4, 16, 8, 64),  # large
        ]

        for batch_size, seq_len, num_heads, head_dim in test_cases:
            hidden_size = num_heads * head_dim

            model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
            x = jnp.ones((batch_size, seq_len, hidden_size))
            mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

            key = jax.random.PRNGKey(42)
            params = model.init(key, x, mask)
            output = model.apply(params, x, mask)

            assert output.shape == (batch_size, seq_len, hidden_size)

    def test_causal_masking_mha(self):
        """Test MultiHeadAttention with causal masking."""
        batch_size, seq_len, num_heads, head_dim = 1, 4, 4, 16
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        x = (
            jnp.arange(batch_size * seq_len * hidden_size)
            .reshape(batch_size, seq_len, hidden_size)
            .astype(jnp.float32)
        )

        # Create causal mask
        causal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))
        mask = jnp.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))

        key = jax.random.PRNGKey(123)
        params = model.init(key, x, mask)
        output = model.apply(params, x, mask)

        assert output.shape == (batch_size, seq_len, hidden_size)
        assert jnp.all(jnp.isfinite(output))

    def test_hidden_size_assertion(self):
        """Test that hidden_size must equal num_heads * head_dim."""
        num_heads, head_dim = 8, 32
        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)

        # Correct hidden size
        correct_hidden_size = num_heads * head_dim
        x_correct = jnp.ones((1, 4, correct_hidden_size))
        mask = jnp.ones((1, 4, 4), dtype=bool)

        key = jax.random.PRNGKey(0)
        params = model.init(key, x_correct, mask)

        # This should work
        output = model.apply(params, x_correct, mask)
        assert output.shape == (1, 4, correct_hidden_size)

        # Wrong hidden size should raise assertion error
        wrong_hidden_size = correct_hidden_size + 1
        x_wrong = jnp.ones((1, 4, wrong_hidden_size))

        with pytest.raises(AssertionError):
            model.apply(params, x_wrong, mask)

    def test_parameter_shapes(self):
        """Test that model parameters have correct shapes."""
        num_heads, head_dim = 6, 32
        hidden_size = num_heads * head_dim
        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)

        x = jnp.ones((2, 4, hidden_size))
        mask = jnp.ones((2, 4, 4), dtype=bool)

        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask)

        # Check parameter shapes
        assert params["params"]["q_proj"]["kernel"].shape == (hidden_size, hidden_size)
        assert params["params"]["k_proj"]["kernel"].shape == (hidden_size, hidden_size)
        assert params["params"]["v_proj"]["kernel"].shape == (hidden_size, hidden_size)
        assert params["params"]["o_proj"]["kernel"].shape == (hidden_size, hidden_size)

    def test_different_inputs_produce_different_outputs(self):
        """Test that different inputs produce different outputs."""
        batch_size, seq_len, num_heads, head_dim = 1, 3, 4, 8
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        # Create different inputs
        x1 = jnp.ones((batch_size, seq_len, hidden_size))
        x2 = jnp.ones((batch_size, seq_len, hidden_size)) * 2.0

        key = jax.random.PRNGKey(0)
        params = model.init(key, x1, mask)

        output1 = model.apply(params, x1, mask)
        output2 = model.apply(params, x2, mask)

        # Outputs should be different for different inputs
        assert not jnp.allclose(output1, output2)

    def test_gradient_flow(self):
        """Test that gradients can flow through the model."""
        batch_size, seq_len, num_heads, head_dim = 1, 2, 2, 4
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)

        key = jax.random.PRNGKey(0)
        x = jax.random.normal(key, (batch_size, seq_len, hidden_size))
        mask = jnp.tril(jnp.ones((batch_size, seq_len, seq_len), dtype=bool))

        def loss_fn(params):
            output = model.apply(params, x, mask)
            return jnp.sum(output**2)

        params = model.init(key, x, mask)

        # Compute gradients
        loss, grads = jax.value_and_grad(loss_fn)(params)

        # Check that gradients exist and are finite
        assert jnp.isfinite(loss)
        for param_name in grads["params"]:
            for weight_name in grads["params"][param_name]:
                grad = grads["params"][param_name][weight_name]
                assert jnp.all(jnp.isfinite(grad))
                # At least some gradients should be non-zero
                assert jnp.any(grad != 0)

class TestMultiHeadAttentionWithRoPE:
    def test_rope_integration(self):
        """Test forward pass with RoPE positional encodings."""
        batch_size, seq_len, num_heads, head_dim = 2, 8, 4, 64
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        # Create inputs with positions
        x = jax.random.normal(jax.random.PRNGKey(42), (batch_size, seq_len, hidden_size))
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)
        position = jnp.arange(seq_len)[None, :].repeat(batch_size, axis=0)

        # Initialize parameters
        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask, position)

        # Forward pass with RoPE
        output_with_rope = model.apply(params, x, mask, position)
        
        # Forward pass without RoPE
        output_without_rope = model.apply(params, x, mask)

        # Check shapes
        assert output_with_rope.shape == (batch_size, seq_len, hidden_size)
        assert output_without_rope.shape == (batch_size, seq_len, hidden_size)
        
        # Outputs should be different (RoPE changes the computation)
        assert not jnp.allclose(output_with_rope, output_without_rope)
        assert jnp.all(jnp.isfinite(output_with_rope))

    def test_custom_positions(self):
        """Test attention with custom position indices."""
        batch_size, seq_len, num_heads, head_dim = 1, 6, 2, 32
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        x = jax.random.normal(jax.random.PRNGKey(123), (batch_size, seq_len, hidden_size))
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)
        
        # Test different position patterns
        sequential_pos = jnp.arange(seq_len)[None, :]
        reverse_pos = jnp.arange(seq_len)[::-1][None, :]
        custom_pos = jnp.array([[0, 2, 4, 1, 3, 5]])  # Non-sequential

        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask, sequential_pos)

        output_seq = model.apply(params, x, mask, sequential_pos)
        output_rev = model.apply(params, x, mask, reverse_pos)
        output_custom = model.apply(params, x, mask, custom_pos)

        # All should have same shape but different values
        assert output_seq.shape == output_rev.shape == output_custom.shape
        assert not jnp.allclose(output_seq, output_rev)
        assert not jnp.allclose(output_seq, output_custom)

    def test_different_sequence_lengths(self):
        """Test attention with various sequence lengths."""
        num_heads, head_dim = 4, 32
        hidden_size = num_heads * head_dim
        
        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        for seq_len in [1, 4, 16, 32]:
            batch_size = 2
            x = jax.random.normal(jax.random.PRNGKey(seq_len), (batch_size, seq_len, hidden_size))
            mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)
            position = jnp.arange(seq_len)[None, :].repeat(batch_size, axis=0)

            key = jax.random.PRNGKey(0)
            params = model.init(key, x, mask, position)
            output = model.apply(params, x, mask, position)

            assert output.shape == (batch_size, seq_len, hidden_size)
            assert jnp.all(jnp.isfinite(output))

    def test_causal_mask_with_rope(self):
        """Test causal attention mask with RoPE."""
        batch_size, seq_len, num_heads, head_dim = 2, 8, 4, 32
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        x = jax.random.normal(jax.random.PRNGKey(999), (batch_size, seq_len, hidden_size))
        position = jnp.arange(seq_len)[None, :].repeat(batch_size, axis=0)
        
        # Create causal mask (lower triangular)
        causal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))
        causal_mask = jnp.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))
        
        # Full attention mask
        full_mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        key = jax.random.PRNGKey(0)
        params = model.init(key, x, full_mask, position)

        output_causal = model.apply(params, x, causal_mask, position)
        output_full = model.apply(params, x, full_mask, position)

        # Should produce different outputs
        assert not jnp.allclose(output_causal, output_full)
        assert jnp.all(jnp.isfinite(output_causal))

    def test_batch_position_independence(self):
        """Test that different batches with same positions produce same results."""
        batch_size, seq_len, num_heads, head_dim = 3, 5, 2, 16
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        # Same input for all batches
        x_single = jax.random.normal(jax.random.PRNGKey(777), (1, seq_len, hidden_size))
        x_batch = jnp.repeat(x_single, batch_size, axis=0)
        
        mask_single = jnp.ones((1, seq_len, seq_len), dtype=bool)
        mask_batch = jnp.repeat(mask_single, batch_size, axis=0)
        
        position_single = jnp.arange(seq_len)[None, :]
        position_batch = jnp.repeat(position_single, batch_size, axis=0)

        key = jax.random.PRNGKey(0)
        params = model.init(key, x_batch, mask_batch, position_batch)

        output_single = model.apply(params, x_single, mask_single, position_single)
        output_batch = model.apply(params, x_batch, mask_batch, position_batch)

        # Each batch element should match the single example
        for i in range(batch_size):
            assert jnp.allclose(output_single[0], output_batch[i], rtol=1e-5)

    def test_position_extrapolation(self):
        """Test behavior with positions beyond training range."""
        batch_size, seq_len, num_heads, head_dim = 1, 4, 2, 32
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        x = jax.random.normal(jax.random.PRNGKey(444), (batch_size, seq_len, hidden_size))
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)
        
        # Normal positions
        normal_pos = jnp.array([[0, 1, 2, 3]])
        
        # Large positions (extrapolation test)
        large_pos = jnp.array([[100, 101, 102, 103]])

        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask, normal_pos)

        output_normal = model.apply(params, x, mask, normal_pos)
        output_large = model.apply(params, x, mask, large_pos)

        # Should handle large positions gracefully
        assert jnp.all(jnp.isfinite(output_large))
        assert not jnp.allclose(output_normal, output_large)

    def test_zero_positions(self):
        """Test attention with all zero positions."""
        batch_size, seq_len, num_heads, head_dim = 2, 4, 2, 16
        hidden_size = num_heads * head_dim

        model = MultiHeadAttention(num_heads=num_heads, head_dim=head_dim)
        
        x = jax.random.normal(jax.random.PRNGKey(555), (batch_size, seq_len, hidden_size))
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)
        
        # All positions are zero
        zero_pos = jnp.zeros((batch_size, seq_len), dtype=jnp.int32)
        normal_pos = jnp.arange(seq_len)[None, :].repeat(batch_size, axis=0)

        key = jax.random.PRNGKey(0)
        params = model.init(key, x, mask, normal_pos)

        output_zero = model.apply(params, x, mask, zero_pos)
        output_normal = model.apply(params, x, mask, normal_pos)

        assert jnp.all(jnp.isfinite(output_zero))
        # Zero positions should give different results than sequential
        assert not jnp.allclose(output_zero, output_normal)
