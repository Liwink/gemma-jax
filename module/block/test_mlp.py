"""
Tests generated by Claude.
"""

import jax
import jax.numpy as jnp
import pytest
import flax.linen as nn
from .mlp import MLP


class TestMLP:

    @pytest.fixture
    def mlp_model(self):
        """Create a standard MLP model for testing."""
        return MLP(hidden_size=512, ffn_dim=2048)

    @pytest.fixture
    def sample_input(self):
        """Create sample input tensor."""
        key = jax.random.PRNGKey(42)
        return jax.random.normal(key, (2, 10, 512))  # (batch=2, seq=10, hidden=512)

    def test_output_shape(self, mlp_model, sample_input):
        """Test that MLP outputs correct shape."""
        key = jax.random.PRNGKey(0)
        params = mlp_model.init(key, sample_input)
        output = mlp_model.apply(params, sample_input)

        # Output should have same shape as input
        assert output.shape == sample_input.shape
        assert output.shape == (2, 10, 512)

    def test_parameter_initialization(self, mlp_model, sample_input):
        """Test that all required parameters are initialized with correct shapes."""
        key = jax.random.PRNGKey(0)
        params = mlp_model.init(key, sample_input)

        # Check parameter structure
        assert "params" in params
        param_dict = params["params"]

        # Check all three projection layers exist
        assert "gate_proj" in param_dict
        assert "up_proj" in param_dict
        assert "down_proj" in param_dict

        # Check weight shapes (no bias since use_bias=False)
        assert param_dict["gate_proj"]["kernel"].shape == (
            512,
            2048,
        )  # hidden_size -> ffn_dim
        assert param_dict["up_proj"]["kernel"].shape == (
            512,
            2048,
        )  # hidden_size -> ffn_dim
        assert param_dict["down_proj"]["kernel"].shape == (
            2048,
            512,
        )  # ffn_dim -> hidden_size

        # Verify no bias parameters
        assert "bias" not in param_dict["gate_proj"]
        assert "bias" not in param_dict["up_proj"]
        assert "bias" not in param_dict["down_proj"]

    def test_different_input_sizes(self, mlp_model):
        """Test MLP with various input dimensions."""
        key = jax.random.PRNGKey(0)

        test_cases = [
            (1, 5, 512),  # Single batch, short sequence
            (4, 20, 512),  # Multiple batches, longer sequence
            (1, 1, 512),  # Single token
            (8, 100, 512),  # Large batch, long sequence
        ]

        for batch_size, seq_len, hidden_size in test_cases:
            input_tensor = jax.random.normal(key, (batch_size, seq_len, hidden_size))
            params = mlp_model.init(key, input_tensor)
            output = mlp_model.apply(params, input_tensor)

            assert output.shape == (batch_size, seq_len, hidden_size)
            assert not jnp.isnan(
                output
            ).any(), f"NaN found in output for shape {input_tensor.shape}"

    def test_geglu_activation_properties(self, mlp_model, sample_input):
        """Test properties of GeGLU activation function."""
        key = jax.random.PRNGKey(0)
        params = mlp_model.init(key, sample_input)

        # Test with zero input
        zero_input = jnp.zeros_like(sample_input)
        zero_output = mlp_model.apply(params, zero_input)

        # GeGLU should output close to zero for zero input (due to GELU properties)
        assert jnp.allclose(zero_output, 0.0, atol=1e-6)

        # Test non-linearity: f(a*x) != a*f(x) for non-zero a
        scaled_input = 2.0 * sample_input
        output_original = mlp_model.apply(params, sample_input)
        output_scaled = mlp_model.apply(params, scaled_input)

        # Should not be linear (due to GELU activation)
        assert not jnp.allclose(output_scaled, 2.0 * output_original, rtol=1e-3)

    def test_gradient_flow(self, mlp_model, sample_input):
        """Test that gradients can flow through the MLP."""
        key = jax.random.PRNGKey(0)
        params = mlp_model.init(key, sample_input)

        def loss_fn(params, x):
            output = mlp_model.apply(params, x)
            return jnp.mean(output**2)  # Simple L2 loss

        # Compute gradients
        loss_value, grads = jax.value_and_grad(loss_fn)(params, sample_input)

        # Check that loss is finite
        assert jnp.isfinite(loss_value)
        assert loss_value > 0  # Should be positive for non-zero output

        # Check that gradients exist and are finite for all parameters
        param_dict = grads["params"]
        for layer_name in ["gate_proj", "up_proj", "down_proj"]:
            grad_kernel = param_dict[layer_name]["kernel"]
            assert not jnp.isnan(grad_kernel).any(), f"NaN gradient in {layer_name}"
            assert not jnp.isinf(grad_kernel).any(), f"Inf gradient in {layer_name}"
            assert jnp.any(
                grad_kernel != 0
            ), f"Zero gradients in {layer_name} (potential dead neurons)"
