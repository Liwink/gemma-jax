"""
Tests generated by Claude Code.
"""

import jax
import jax.numpy as jnp
import pytest
from attention.attention import scaled_dot_product_attention, MASK_VALUE


class TestScaledDotProductAttention:
    def test_basic_functionality(self):
        """Test basic attention computation with simple inputs."""
        batch_size, num_heads, seq_len, head_dim = 2, 4, 8, 16

        # Create test inputs
        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )

        # No masking (all True)
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Check output shape
        expected_shape = (batch_size, num_heads, seq_len, head_dim)
        assert result.shape == expected_shape

        # Check that result is finite
        assert jnp.all(jnp.isfinite(result))

    def test_output_shape_consistency(self):
        """Test that output shape is consistent across different input sizes."""
        test_cases = [
            (1, 2, 4, 8),  # small
            (2, 8, 12, 32),  # medium
            (4, 16, 8, 64),  # large
        ]

        for batch_size, num_heads, seq_len, head_dim in test_cases:
            q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            v = jnp.ones((batch_size, num_heads, seq_len, head_dim))
            mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

            result = scaled_dot_product_attention(q, k, v, mask)
            expected_shape = (batch_size, num_heads, seq_len, head_dim)
            assert result.shape == expected_shape

    def test_attention_scaling(self):
        """Test that attention scores are properly scaled by sqrt(head_dim)."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create inputs with known values
        q = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]]])
        k = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]]]])
        v = jnp.array([[[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # With orthogonal q and k, attention should be uniform after softmax
        # Check that result is reasonable
        assert jnp.all(jnp.isfinite(result))
        assert result.shape == (1, 1, 2, 4)

    def test_causal_masking(self):
        """Test causal (lower triangular) masking."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 3, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )

        # Create causal mask (lower triangular)
        causal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))
        mask = jnp.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (batch_size, num_heads, seq_len, head_dim)
        assert jnp.all(jnp.isfinite(result))

    def test_full_masking(self):
        """Test behavior with fully masked attention."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = jnp.ones((batch_size, num_heads, seq_len, head_dim))

        # All positions masked out
        mask = jnp.zeros((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Result should still have correct shape
        assert result.shape == (batch_size, num_heads, seq_len, head_dim)

        # With all positions masked, softmax should produce uniform weights
        # over masked positions, but the exact behavior depends on implementation
        assert jnp.all(jnp.isfinite(result))

    def test_identity_attention(self):
        """Test attention when q, k, v are designed for identity mapping."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create inputs where each position attends only to itself
        q = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]]])
        k = jnp.array([[[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]]])
        v = jnp.array([[[[10.0, 20.0, 30.0, 40.0], [50.0, 60.0, 70.0, 80.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (1, 1, 2, 4)
        assert jnp.all(jnp.isfinite(result))

    def test_batch_independence(self):
        """Test that different batches are processed independently."""
        batch_size, num_heads, seq_len, head_dim = 3, 2, 2, 4

        # Create different inputs for each batch
        q = (
            jnp.arange(batch_size * seq_len * num_heads * head_dim)
            .reshape(batch_size, num_heads, seq_len, head_dim)
            .astype(jnp.float32)
        )
        k = q + 1.0
        v = q + 2.0

        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        # Process each batch individually and compare
        for i in range(batch_size):
            single_result = scaled_dot_product_attention(
                q[i : i + 1], k[i : i + 1], v[i : i + 1], mask[i : i + 1]
            )
            assert jnp.allclose(result[i : i + 1], single_result, rtol=1e-5)

    def test_attention_weights_sum_to_one(self):
        """Test that attention weights sum to 1 for unmasked positions."""
        batch_size, num_heads, seq_len, head_dim = 1, 1, 3, 4

        q = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        k = jnp.ones((batch_size, num_heads, seq_len, head_dim))
        v = jnp.ones((batch_size, num_heads, seq_len, head_dim))

        # Test with partial masking
        mask = jnp.array(
            [[[True, True, False], [True, True, True], [True, False, True]]]
        )

        # We can't directly test attention weights since they're internal,
        # but we can test that the function runs correctly
        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (batch_size, num_heads, seq_len, head_dim)
        assert jnp.all(jnp.isfinite(result))

    def test_different_qkv_values(self):
        """Test attention with different q, k, v values."""
        batch_size, num_heads, seq_len, head_dim = 1, 2, 1, 4

        # Create distinct q, k, v
        q = jnp.array([[[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]]]])
        k = jnp.array([[[[2.0, 1.0, 4.0, 3.0], [6.0, 5.0, 8.0, 7.0]]]])
        v = jnp.array([[[[10.0, 20.0, 30.0, 40.0], [50.0, 60.0, 70.0, 80.0]]]])
        mask = jnp.ones((batch_size, seq_len, seq_len), dtype=bool)

        result = scaled_dot_product_attention(q, k, v, mask)

        assert result.shape == (1, 1, 2, 4)
        assert jnp.all(jnp.isfinite(result))

        # Result should be different from input values
        assert not jnp.allclose(result, q)
        assert not jnp.allclose(result, k)
        assert not jnp.allclose(result, v)

    def test_mask_value_constant(self):
        """Test that MASK_VALUE is properly defined."""
        assert MASK_VALUE == -1e10
        assert isinstance(MASK_VALUE, (int, float))
        assert MASK_VALUE < 0
